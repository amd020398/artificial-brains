# -*- coding: utf-8 -*-
"""Recommender_system.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G9DPG9X0kYOr64YOG9leKHEkV_ARXu7v
"""

import pandas as pd
import numpy as np

# foodexp = pd.read_csv("/content/drive/MyDrive/BWAI Challenge 3 Dataset/foodexp.csv")
# foodfridge = pd.read_csv("/content/drive/MyDrive/BWAI Challenge 3 Dataset/foodfridge.csv")
# foodshelf = pd.read_csv("/content/drive/MyDrive/BWAI Challenge 3 Dataset/foodshelf.csv")
# foodsimple = pd.read_csv("/content/drive/MyDrive/BWAI Challenge 3 Dataset/foodsimple.csv")

# combined_food_df = pd.concat([foodexp, foodfridge, foodshelf,
#                                            foodsimple]).drop_duplicates()

from google.colab import drive
drive.mount('/content/drive')

food_consumed = pd.read_csv("/content/drive/MyDrive/BWAI Challenge 3 Dataset/orderItemRequest_cleaned.csv")
# orderHeaderRequests

food_consumed['OrderItemId'] = food_consumed.OrderItemId.astype('string')
food_consumed['OrderId'] = food_consumed.OrderId.astype('string')

food_consumed["user_id"] = food_consumed["OrderId"] + food_consumed["OrderItemId"]

food_consumed.head()

food_consumed.info()

food_consumed.dtypes

df = food_consumed[(food_consumed['OrderItemId'] == 4)]
df

# order id , product item, product type, product fequency 
# save all the unique rows of product item 
# food_consumed["ProductItem"].unique()



food_consumed_ = food_consumed.drop(columns=["OrderId","OrderItemId","DETAILS","STORAGE"])

food_consumed_['PERIOD'].map(food_consumed_['PERIOD'].value_counts())

food_consumed_['product fequency'] = food_consumed_['ITEM'].map(food_consumed_['ITEM'].value_counts())
food_consumed_

# filt = {1:range(0,5), 2:range(6,10), 3:range(11,15), 4:range(16,20), 5:range(21,25), 6:range(26,30), 7:range(31,35), 8:range(36,40), 9:range(41,45), 10:range(46,52)}
# food_consumed_['score'] = food_consumed_['product fequency'].map(filt)

# 1:range(0,5), 2:range(6,10), 3:range(11,15), 4:range(16,20), 5:range(21,25), 6:range(26,30), 7:range(31,35), 8:range(36,40), 9:range(41,45), 10:range(46,52)
criteria = [food_consumed_['PERIOD'].between(901,1000), 
            food_consumed_['PERIOD'].between(801,900), 
            food_consumed_['PERIOD'].between(701,800),
            food_consumed_['PERIOD'].between(601,700),
            food_consumed_['PERIOD'].between(301,600),
            food_consumed_['PERIOD'].between(201,300),
            food_consumed_['PERIOD'].between(81,200),
            food_consumed_['PERIOD'].between(10.1,80),
            food_consumed_['PERIOD'].between(5.1,10),
            food_consumed_['PERIOD'].between(0,5)
            ]

values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

food_consumed_['score'] = np.select(criteria, values, 0)

food_consumed_

df = food_consumed_[(food_consumed_['score'] == 5)]
df.tail()

# food_consumed_.set_index('OrderId', inplace = True)

df_final = food_consumed_.drop(columns="product fequency")
df_final

!pip3 install plotly 
!pip3 install surprise

from surprise import Dataset,Reader

reader = Reader(rating_scale=(1, 10))
data = Dataset.load_from_df(df_final[['user_id', 'ITEM', 'score']], reader)

# Split data to train and test
from surprise.model_selection import train_test_split
trainset, testset = train_test_split(data, test_size=.25,random_state=123)

trainset.all_ratings()

# However the ids are the inner ids and not the raw ids
# raw ids can be obatined as follows

print(trainset.to_raw_uid(0))
# print(trainset.to_raw_uid(1066))

from surprise import SVD, KNNWithMeans
from surprise import accuracy

svd_model = SVD(n_factors=5,biased=False)
svd_model.fit(trainset)

testset[0]

test_pred = svd_model.test(testset)
# test_pred

# compute RMSE
accuracy.rmse(test_pred)

from surprise import BaselineOnly
from surprise.model_selection import cross_validate

print('Using ALS')
bsl_options = {'method': 'als',
               'n_epochs': 5,
               'reg_u': 12,
               'reg_i': 5
               }
algo = BaselineOnly(bsl_options=bsl_options)
cross_validate(algo, data, measures=['RMSE','MAE'], cv=3, verbose=False)

from collections import defaultdict

from surprise.model_selection import KFold


def precision_recall_at_k(predictions, k=20, threshold=3.5):
    """Return precision and recall at k metrics for each user"""

    # First map the predictions to each user.
    user_est_true = defaultdict(list)
    for uid, _, true_r, est, _ in predictions:
        user_est_true[uid].append((est, true_r))

    precisions = dict()
    recalls = dict()
    for uid, user_ratings in user_est_true.items():

        # Sort user ratings by estimated value
        user_ratings.sort(key=lambda x: x[0], reverse=True)

        # Number of relevant items
        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)

        # Number of recommended items in top k
        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])

        # Number of relevant and recommended items in top k
        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))
                              for (est, true_r) in user_ratings[:k])

        # Precision@K: Proportion of recommended phone that are relevant
        # When n_rec_k is 0, Precision is undefined. We here set it to 0.

        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0

        # Recall@K: Proportion of relevant phone that are recommended
        # When n_rel is 0, Recall is undefined. We here set it to 0.

        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0

    return precisions, recalls



kf = KFold(n_splits=20)
for trainset, testset in kf.split(data):
    algo.fit(trainset)
    predictions = algo.test(testset)
    precisions, recalls = precision_recall_at_k(predictions, k=20, threshold=4)

    # Precision and recall can then be averaged over all users
    print(sum(prec for prec in precisions.values()) / len(precisions))
    print(sum(rec for rec in recalls.values()) / len(recalls))

from surprise import SVD
from surprise import Dataset


def get_top_n(predictions, n=20):
    """Return the top-N recommendation for each user from a set of predictions.

    Args:
        predictions(list of Prediction objects): The list of predictions, as
            returned by the test method of an algorithm.
        n(int): The number of recommendation to output for each user. Default
            is 10.

    Returns:
    A dict where keys are user (raw) ids and values are lists of tuples:
        [(raw item id, rating estimation), ...] of size n.
    """

    # First map the predictions to each user.
    top_n = defaultdict(list)
    for uid, iid, true_r, est, _ in predictions:
        top_n[uid].append((iid, est))

    # Then sort the predictions for each user and retrieve the k highest ones.
    for uid, user_ratings in top_n.items():
        user_ratings.sort(key=lambda x: x[1], reverse=True)
        top_n[uid] = user_ratings[:n]

    return top_n


# First train an BaselineOnly algorithm on the smartphone dataset.
trainset = data.build_full_trainset()
algo.fit(trainset)

# Than predict ratings for all pairs (u, i) that are NOT in the training set.
testset = trainset.build_anti_testset()
predictions = algo.test(testset)

top_n = get_top_n(predictions, n=20)

# Print the recommended items for each user
for uid, user_ratings in top_n.items():
    print(uid, [iid for (iid, _) in user_ratings])

import pickle

pickle.dump(algo, open("model.pkl",'wb'))

# To load the model from disk
# loaded_model = pickle.load(open(filename, 'rb'))